{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "WARNING:tensorflow:From /Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from PONG_NN_Weights_FINAL/network-220000\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/RMSProp not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-1-aee0a67c7181>:129) ]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/andrew/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/andrew/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-aee0a67c7181>\", line 410, in <module>\n    player = DQNAgent()\n  File \"<ipython-input-1-aee0a67c7181>\", line 129, in __init__\n    self.saver = tf.train.Saver()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/RMSProp not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-1-aee0a67c7181>:129) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key Variable/RMSProp not found in checkpoint\n\t [[{{node save/RestoreV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1276\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key Variable/RMSProp not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-1-aee0a67c7181>:129) ]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/andrew/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/andrew/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-aee0a67c7181>\", line 410, in <module>\n    player = DQNAgent()\n  File \"<ipython-input-1-aee0a67c7181>\", line 129, in __init__\n    self.saver = tf.train.Saver()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Key Variable/RMSProp not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-1-aee0a67c7181>:129) ]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1285\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   object_graph_string = reader.get_tensor(\n\u001b[0;32m-> 1591\u001b[0;31m       checkpointable.OBJECT_GRAPH_PROTO_KEY)\n\u001b[0m\u001b[1;32m   1592\u001b[0m   object_graph_proto = (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    369\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[0;32m--> 370\u001b[0;31m                                           status)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aee0a67c7181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m     \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;31m# call the start method in the parent classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-aee0a67c7181>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, checkpoint_path, playback_mode, verbose_logging)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Successfully loaded:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1292\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/RMSProp not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-1-aee0a67c7181>:129) ]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/andrew/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/andrew/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/Users/andrew/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-aee0a67c7181>\", line 410, in <module>\n    player = DQNAgent()\n  File \"<ipython-input-1-aee0a67c7181>\", line 129, in __init__\n    self.saver = tf.train.Saver()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/Users/andrew/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/RMSProp not found in checkpoint\n\t [[node save/RestoreV2 (defined at <ipython-input-1-aee0a67c7181>:129) ]]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "\n",
    "The methods in the code are based on Danielslater at https://github.com/DanielSlater/PyGamePlayer \n",
    "and Akshay Srivatsan's work at https://github.com/asrivat1/DeepLearningVideoGames\n",
    "\n",
    "'''\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from pong_player import PongPlayer\n",
    "from pygame_player import PyGamePlayer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pygame.constants import K_DOWN, K_UP\n",
    "\n",
    "class DQNAgent(PongPlayer):\n",
    "    \n",
    "    ''' \n",
    "    This class is used to define the methods of the q-learning agent.  \n",
    "    It inherits from the PongPlayer class and forwards requests to that praent class.\n",
    "    The Pongplayer class is also a descendant class of the PyGamePlayer class.\n",
    "    The PyGamePlayer and PongPlayer classes were created by Daniel Slater.\n",
    "    https://github.com/DanielSlater/PyGamePlayer\n",
    "    \n",
    "    These classes allow for interaction between a learning agent and a game environment \n",
    "    without the need to interact directly with the PyGame code.\n",
    "    \n",
    "    The PyGamePlayer class is important because it extracts observations about the game state and environment.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # We have to define some parameters for the Q-Learning Algorithim\n",
    "\n",
    "    resized_state_x, resized_state_y = (84, 84) # Pixel resolution of the grame screen (environment state dimension)\n",
    "\n",
    "    state_frames = 4 # number of frames to store in one state\n",
    "\n",
    "    action_size = 3 # How large is the action space of the agent? PONG: up, down, still\n",
    "\n",
    "    gamma = 0.95  # how much to discount future rewards\n",
    "                  # weight upcomming actions more heavily than ones in the distant future\n",
    "\n",
    "    epsilon_initial = 1.0 # Initial probability that the agent will choose random action\n",
    "                          # Encourages initial exploration and decreases over time to favour exploitation\n",
    "\n",
    "    epsilon_min = 0.01 # Final probability that the agent will choose random action\n",
    "    \n",
    "    learning_rate = 1e-6 # stochastic gradient descent optimizer step size\n",
    "\n",
    "    explore_steps = 100000 # frames over which to anneal epsilon\n",
    "\n",
    "    minibatch_size = 32 # minibatch which is a random sample from memory\n",
    "                        # observations from these batches will be used to train the NN\n",
    "\n",
    "    memory = 50000 # How many observations do we want to store in memory to be sampled? (memory size)\n",
    "\n",
    "    observation_steps = 40000 # How many actions the agents takes before q-learning algorithim kicks in \n",
    "\n",
    "    savepoints = 10000 # Save neural network weights every _ steps\n",
    "    \n",
    "    update_time = 10000 # In simple Q-Learning a state-action table would be updated fequently \n",
    "                        # with Q-values.\n",
    "                        # In this case the table is the neural network and the agent decides \n",
    "                        # the next action using the networkm therefore we have to update the\n",
    "                        # network based on more action-states and observations from memory\n",
    "            \n",
    "    def __init__(self, checkpoint_path=\"PONG_NN_Weights_FINAL\", playback_mode=False, verbose_logging=False):\n",
    "\n",
    "        # set the first action to be - do nothing\n",
    "        # this is explained further in the get_keys_pressed function\n",
    "        self.prev_action = np.zeros(self.action_size)\n",
    "        self.prev_action[1] = 1\n",
    "\n",
    "        # Make the previous state empty to be filled in later\n",
    "        self.prev_state = None\n",
    "\n",
    "        # for output logging\n",
    "        self.time = 0\n",
    "\n",
    "        self.epsilon = self.epsilon_initial # self.epsilon will be decayed over time to encourage exploration intitally and exploitation finally\n",
    "\n",
    "        self.observations = deque() # This deque will store all of the observation information at each step\n",
    "                                    # previous state, previous action, reward, next state, terminal \n",
    "                                    # terminal = True if we have reached a terminal state, meaning the next frame will be a restart\n",
    "                                    # In the case of Pong this terminal state will always = False\n",
    "\n",
    "        self.playback_mode = playback_mode # set playback_mode = False to train the Agent\n",
    "                                           # set playback_mode = True and the Agent will play the game with saved NN weights\n",
    "                                           # playback_mode = False by default\n",
    "\n",
    "        self.verbose_logging = verbose_logging # Keep track of the Q-Value output from the neural network\n",
    "\n",
    "        # Super is a shortcut to access the parent class (classes) without giving a name\n",
    "        # We force the game to run at 8 frames per second and define the playback mode setting\n",
    "        # This calls methods from the PyGamePlayer class script\n",
    "        super(DQNAgent, self).__init__(force_game_fps=8, run_real_time=playback_mode)\n",
    "\n",
    "        # initialize the Action Q Network\n",
    "        self.input_layer,self.output_layer,self.W_conv1,self.b_conv1,self.W_conv2,self.b_conv2,self.W_conv3,self.b_conv3,self.W_fc1,self.b_fc1,self.W_fc2,self.b_fc2 = DQNAgent.create_network()\n",
    "\n",
    "        # initialize Target Q Network\n",
    "        self.input_layerT,self.output_layerT,self.W_conv1T,self.b_conv1T,self.W_conv2T,self.b_conv2T,self.W_conv3T,self.b_conv3T,self.W_fc1T,self.b_fc1T,self.W_fc2T,self.b_fc2T = DQNAgent.create_network()\n",
    "\n",
    "        # Used to update the neural network weights and biases\n",
    "        # see the train method and copy target network method\n",
    "        self.copyTargetQNetworkOperation = [self.W_conv1T.assign(self.W_conv1),self.b_conv1T.assign(self.b_conv1),self.W_conv2T.assign(self.W_conv2),self.b_conv2T.assign(self.b_conv2),self.W_conv3T.assign(self.W_conv3),self.b_conv3T.assign(self.b_conv3),self.W_fc1T.assign(self.W_fc1),self.b_fc1T.assign(self.b_fc1),self.W_fc2T.assign(self.W_fc2),self.b_fc2T.assign(self.b_fc2)]\n",
    "        \n",
    "        # Set action and target placeholders\n",
    "        # used to train the agent to associate certain actions in specific states lead to specific rewards\n",
    "        self.action = tf.placeholder(\"float\", [None, self.action_size]) #actioninput\n",
    "        self.target = tf.placeholder(\"float\", [None]) #yinput\n",
    "\n",
    "        # Minimize the cost of the neural network while training\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.output_layer, self.action), reduction_indices=1)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.target - Q_action))\n",
    "        self.train_agent = tf.train.RMSPropOptimizer(1e-6, decay=0.99, momentum=0.0, epsilon=1e-10).minimize(self.cost)        \n",
    "\n",
    "        # Save neural network weights to determine Agent's next action\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        \n",
    "        # self.session = tf.Session() # Instantiate the tensorflow session \n",
    "        self.session = tf.InteractiveSession()\n",
    "        \n",
    "        # Every used variable in the network needs to be initialized\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Save neural netowrk weights for playback\n",
    "        self.saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                self.saver.restore(self.session, checkpoint.model_checkpoint_path)\n",
    "                print(\"Successfully loaded:\", checkpoint.model_checkpoint_path) # output information\n",
    "        else:\n",
    "                print(\"Could not find old network weights\") # output information\n",
    "\n",
    "    def get_keys_pressed(self, screen_array, reward, terminal):\n",
    "        \n",
    "        '''\n",
    "        We use the method to:\n",
    "\n",
    "        Get and preprocess the frames/images from the game. \n",
    "         - This includes, converting the image to grayscale from RGB\n",
    "         - Resizing the images from 640 x 480 to a more managable size\n",
    "           for the neural network computation 80 x 80 \n",
    "         - Also stacking the last 4 frames to provide the agent with more \n",
    "           information about the movement of the ball on the Pong screen\n",
    "\n",
    "        Collect information and store in memory.\n",
    "         - Store previous observations in memory to be accessed when training\n",
    "         - This is done so that the agent understands what behavious/actions\n",
    "           yield a reward of 1 (good) or a reward of -1 (bad)\n",
    "\n",
    "        Decide when to start training and what actions to for every next state\n",
    "         - If the length of the observation deque exceeds the set memory size\n",
    "           then start deleting values at the start of the deque to keep it at\n",
    "           the set memory size.\n",
    "         - We se tthe number of observation steps above, so once the length of the \n",
    "           observation deque exceeds that length the observation phase is over and \n",
    "           the model begins to train\n",
    "         - The next state is updated to be the previous state and we use the previous action \n",
    "           to determine the next action using the choose_next_action and key_presses_from_action\n",
    "           methdods\n",
    "    \n",
    "        '''\n",
    "        \n",
    "        # preprocess image data to be fed into neural network to grayscale\n",
    "        # resize image to 84x84 (7056 pixels)\n",
    "        screen_resized_grayscaled = cv2.cvtColor(cv2.resize(screen_array, (self.resized_state_x, self.resized_state_y)),cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # set the pixels to all be 0 or 1 \n",
    "        threshold, screen_resized_binary = cv2.threshold(screen_resized_grayscaled, 1, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "        # first frame must be handled differently\n",
    "        # we specify an empty previous state to start\n",
    "        if self.prev_state is None:\n",
    "            # the previous state will contain the image data from the previous 4 frames as set above by self.state_frames \n",
    "            self.prev_state = np.stack(tuple(screen_resized_binary for _ in range(self.state_frames)), axis=2)\n",
    "            return DQNAgent.key_presses_from_action(self.prev_action)\n",
    "        \n",
    "        # *****\n",
    "        screen_resized_binary = np.reshape(screen_resized_binary, (self.resized_state_x, self.resized_state_y, 1))\n",
    "        next_state = np.append(self.prev_state[:, :, 1:], screen_resized_binary, axis=2)\n",
    "        \n",
    "        if not self.playback_mode:\n",
    "            # store the transition in previous_observations\n",
    "            self.observations.append((self.prev_state, self.prev_action, reward, next_state, terminal))\n",
    "            # if the length of the observations deque grows to be larger than the set memory size\n",
    "            # then start letting go of the oldest observations and xontinue to add the newst ones\n",
    "            if len(self.observations) > self.memory:\n",
    "                self.observations.popleft()\n",
    "\n",
    "            # If the length of the obersavtion deque exceeds the set amount of observation steps\n",
    "            # then it is time to begin the training process/ Q-value optimization\n",
    "            # and the time counter starts counting\n",
    "            if len(self.observations) > self.observation_steps:\n",
    "                self.train()\n",
    "                self.time += 1\n",
    "                \n",
    "        # update the state\n",
    "        self.prev_state = next_state\n",
    "        \n",
    "        # Use the choose_next_action method to update the next action\n",
    "        self.prev_action = self.choose_next_action()\n",
    "\n",
    "        if not self.playback_mode:\n",
    "            # gradually reduce the probability of a random action\n",
    "            \n",
    "            # this is controlled by the number of explore steps\n",
    "            \n",
    "            # the number of observation steps controls for how long the agent \n",
    "            # chooses random actions while observing the environment\n",
    "            \n",
    "            if self.epsilon > self.epsilon_min and len(self.observations) > self.observation_steps:\n",
    "            # so if epsilon is 1 the agent will always choose a random \n",
    "            # action from the action set\n",
    "            # at the same time, if the observation deque becomes longer \n",
    "            # than the number of set observation steps \n",
    "            #then we reduce the probability of random action using:\n",
    "                self.epsilon -= (self.epsilon_initial - self.epsilon_min) / self.explore_steps\n",
    "\n",
    "            print(\"Time: %s epsilon: %s reward %s\" % (self.time, self.epsilon, reward))\n",
    "                  \n",
    "        # prev_action is = choose_next_action    \n",
    "        # we get the Key Press returned from this method\n",
    "        # UP, DOWN, LEFT OR RIGHT\n",
    "        return DQNAgent.key_presses_from_action(self.prev_action)                                      \n",
    "                  \n",
    "    def choose_next_action(self):    \n",
    "\n",
    "        '''\n",
    "        The agent uses this to decide what next action to take.  This is controlled by epsilon.  \n",
    "        A random number is selcted between 0 and 1.  If it is less than epsilon at the time, \n",
    "        then a random action is chosen.  If it is greater than epsilon then we use the \n",
    "        neural network to predict the next best action bast on the state.\n",
    "        '''       \n",
    "        # the action input to the game is an array of 3 numbers\n",
    "        # [1,0,0] input results in no action by the agent\n",
    "        # [0,1,0] input results in one key press for UP\n",
    "        # [0,0,1] input results in one key press for DOWN\n",
    "                  \n",
    "        # so the new action for the agent to take, determined by the neural network\n",
    "        # is going to be an array [x,x,x]\n",
    "                  \n",
    "        # so set the array to be an array of zeroes, to be adjusted \n",
    "        # when the new action index is determined by the neural network\n",
    "        next_action = np.zeros(self.action_size)\n",
    "        action_index = 0\n",
    "        \n",
    "        # if epsilon < a random number, choose random action\n",
    "        if (not self.playback_mode) and (random.random() <= self.epsilon):\n",
    "            # choose an action randomly\n",
    "            action_index = random.randrange(self.action_size)\n",
    "            \n",
    "        else:\n",
    "            # choose an action useing neural network prediction\n",
    "            Q_values = self.output_layer.eval(feed_dict= {self.input_layer:[self.prev_state]})[0]\n",
    "\n",
    "            # everytime, the neural network makes an action prediction print the q-values          \n",
    "            if self.verbose_logging:\n",
    "                print(\"Action Q-Values are %s\" % Q_values)\n",
    "            action_index = np.argmax(Q_values)\n",
    "            \n",
    "        # use the outout array of the neural network (Q-values) \n",
    "        # to set the correct action input in the\n",
    "        next_action[action_index] = 1\n",
    "        return next_action               \n",
    " \n",
    "    # Used to update the network weights for exploitation of Q-value function\n",
    "    def copyTargetQNetwork(self):\n",
    "        self.session.run(self.copyTargetQNetworkOperation)\n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        '''\n",
    "        This method is used to train the agent to associate state-action pairs with positive or negative rewards.\n",
    "        We take a random sample from meomory (a mini batch), calculate the possible actions the agent could take \n",
    "        in the next state, and then calculate the expected reward using the Bellman Equation.\n",
    "        Those expcted rewards are then used to train the agent to associate certain actions with better rewards\n",
    "\n",
    "        '''\n",
    "        # sample a mini_batch to train on\n",
    "        mini_batch = random.sample(self.observations, self.minibatch_size)\n",
    "        \n",
    "        # the self.observations deque holds arrays of the observations of the game screen \n",
    "        # prev_state, last_action, reward, next_state, terminal\n",
    "        # [   0            1          2         3         4     ]\n",
    "        # the mini_batch randomly samples the memory to get  \n",
    "        prev_states = [d[0] for d in mini_batch]\n",
    "        actions = [d[1] for d in mini_batch]\n",
    "        rewards = [d[2] for d in mini_batch]\n",
    "        next_states = [d[3] for d in mini_batch]\n",
    "        \n",
    "        agents_expected_reward = []\n",
    "        # this gives us the agents expected reward for each action the agent might take\n",
    "        \n",
    "        # this loop does the following:\n",
    "        # first, we feed the next possible states into the neural network and a set of possible actions are returned\n",
    "        # these are Q_values or possible actions \n",
    "        possible_actions = self.output_layerT.eval(feed_dict={self.input_layerT: next_states})\n",
    "   \n",
    "        # Second, we determine if the respective state is the last or terminal state \n",
    "        # (determined by the get_key_presses method) in the PyGamePlayer parent class\n",
    "        # If it is then the expected reward is simply from the same observation array \n",
    "        # as the respective possible action\n",
    "        \n",
    "        for m in range(len(mini_batch)):\n",
    "            if mini_batch[m][4]:\n",
    "                agents_expected_reward.append(rewards[m])\n",
    "                \n",
    "            # If the state we are in is not a terminal state then we calculate the \n",
    "            # reward at current time step + discounted future reward\n",
    "            # This is the Q-Value function\n",
    "            # Instead of update a table of Q-values with state-action pairs \n",
    "            # it will be used to train the Neural Network\n",
    "            else:\n",
    "                agents_expected_reward.append(rewards[m] + self.gamma * np.max(possible_actions[m]))\n",
    "        \n",
    "        # train the agent to associate these actions in these states lead to this reward\n",
    "        # self.action and self.target were set as tensorflow placeholders above\n",
    "        # this is the equilevent of updating the Q-Value table in no-Deep Q-Learning\n",
    "        self.train_agent.run(feed_dict={self.target : agents_expected_reward, self.action : actions, self.input_layer : prev_states})\n",
    "        \n",
    "        # save neural network checkpoints\n",
    "        if self.time % self.savepoints == 0:\n",
    "            self.saver.save(self.session, self.checkpoint_path + '/network', global_step=self.time)\n",
    "            \n",
    "        # update neural netowrk weights and biases for more accurate predictions\n",
    "        if self.time % self.update_time == 0:\n",
    "            self.copyTargetQNetwork()\n",
    "            \n",
    "    \n",
    "    def create_network():\n",
    "        \n",
    "        ''' \n",
    "        Now we can develop the neural network we will use to determine what action the agent should take given a set of 4 states\n",
    "        This neural network was written using Tensorflow, by https://github.com/songrotek/DQN-Atari-Tensorflow.git.\n",
    "        \n",
    "        I am removing the max-pooling layers, as Akshay Srivatsan noted that the layer may have \n",
    "        discarded useful information about the environment state (image), that the agent might find informative.\n",
    "        \n",
    "        There are hidden convolutional layers.\n",
    "        The input is of size 84,84,4 (the state as a stack of 4 images)\n",
    "        The output is the shape of the total number of possible actions\n",
    "        \n",
    "        '''\n",
    "        # Weight and bias variables\n",
    "        W_conv1 = DQNAgent.weight_variable([8,8,4,32])\n",
    "        b_conv1 = DQNAgent.bias_variable([32])\n",
    "\n",
    "        W_conv2 = DQNAgent.weight_variable([4,4,32,64])\n",
    "        b_conv2 = DQNAgent.bias_variable([64])\n",
    "\n",
    "        W_conv3 = DQNAgent.weight_variable([3,3,64,64])\n",
    "        b_conv3 = DQNAgent.bias_variable([64])\n",
    "\n",
    "        W_fc1 = DQNAgent.weight_variable([3136,512])\n",
    "        b_fc1 = DQNAgent.bias_variable([512])\n",
    "\n",
    "        W_fc2 = DQNAgent.weight_variable([512,DQNAgent.action_size])\n",
    "        b_fc2 = DQNAgent.bias_variable([DQNAgent.action_size])\n",
    "\n",
    "        # input layer tensor\n",
    "        input_layer = tf.placeholder(\"float\", [None, DQNAgent.resized_state_x, DQNAgent.resized_state_y, DQNAgent.state_frames])\n",
    "\n",
    "        # hidden layers\n",
    "        h_conv1 = tf.nn.relu(DQNAgent.conv2d(input_layer,W_conv1,4) + b_conv1)\n",
    "        h_conv2 = tf.nn.relu(DQNAgent.conv2d(h_conv1,W_conv2,2) + b_conv2)\n",
    "        h_conv3 = tf.nn.relu(DQNAgent.conv2d(h_conv2,W_conv3,1) + b_conv3)\n",
    "        h_conv3_flat = tf.reshape(h_conv3,[-1,3136])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat,W_fc1) + b_fc1)\n",
    "    \n",
    "        # Q Value layer\n",
    "        output_layer = tf.matmul(h_fc1,W_fc2) + b_fc2\n",
    "        \n",
    "        # Return the input layer and putput layer to be fed into the network\n",
    "        return input_layer,output_layer,W_conv1,b_conv1,W_conv2,b_conv2,W_conv3,b_conv3,W_fc1,b_fc1,W_fc2,b_fc2\n",
    "\n",
    "    # define the shape and tensor for the weight variables\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    # define the shape and tensor for the weight variables\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.01, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    # define the input_layer, weight variable, and tensor for the convolutional layers\n",
    "    def conv2d(x, W, stride):\n",
    "        return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"VALID\")\n",
    "\n",
    "                    \n",
    "    def key_presses_from_action(action_set):\n",
    "        \n",
    "        '''\n",
    "        This function translates the action_set to actual key presses in PyGame \n",
    "\n",
    "        '''\n",
    "        if action_set[0] == 1:\n",
    "            return [K_DOWN]\n",
    "        elif action_set[1] == 1:\n",
    "            return []\n",
    "        elif action_set[2] == 1:\n",
    "            return [K_UP]\n",
    "        raise Exception(\"Unexpected action\")\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    player = DQNAgent()\n",
    "    \n",
    "    # call the start method in the parent classes\n",
    "    player.start()\n",
    "    \n",
    "    # importing pong will start the game playing  \n",
    "    import pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
